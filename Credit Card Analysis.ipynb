{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# credit card transaction fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "training_data_path = \"fraudTrain.csv\"\n",
    "test_data_path = \"fraudTest.csv\"\n",
    "\n",
    "training_data_raw = pd.read_csv(training_data_path)\n",
    "test_data_raw = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning here\n",
    "training_data_clean = {\"cc_num\": [], \"merchant\": [], \"category\": [], \"amt\": [], \"first\": [], \"last\": [], \"gender\": [], \"lat\": [], \"long\": [], \"city_pop\": [], \"job\": [], \"unix_time\": [], \"merch_lat\": [], \"merch_long\": [], \"is_fraud\": []}\n",
    "for idx in range(len(training_data_raw[\"cc_num\"])):\n",
    "    for key in training_data_clean.keys():\n",
    "        training_data_clean[key].append(training_data_raw[key][idx])\n",
    "training_data_clean = pd.DataFrame.from_dict(training_data_clean)\n",
    "training_data_clean.to_csv(\"credit_card_fraud_clean_train.csv\", index=False)\n",
    "\n",
    "test_data_clean = {\"cc_num\": [], \"merchant\": [], \"category\": [], \"amt\": [], \"first\": [], \"last\": [], \"gender\": [], \"lat\": [], \"long\": [], \"city_pop\": [], \"job\": [], \"unix_time\": [], \"merch_lat\": [], \"merch_long\": [], \"is_fraud\": []}\n",
    "for idx in range(len(test_data_raw[\"cc_num\"])):\n",
    "    for key in test_data_clean.keys():\n",
    "        test_data_clean[key].append(test_data_raw[key][idx])\n",
    "test_data_clean = pd.DataFrame.from_dict(test_data_clean)\n",
    "test_data_clean.to_csv(\"credit_card_fraud_clean_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693\n",
      "14\n",
      "Non-categorical data length: 1296675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Jerome/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant data length: 1296675\n",
      "Category length: 1296675\n",
      "Fraud length: 1296675\n",
      "Number of fraudulent transactions: 7506\n"
     ]
    }
   ],
   "source": [
    "#Handle subselection of columns for the neural network\n",
    "#columns to keep: cc_num, merchant, category, amt, lat, long, unix_time, merch_lat, merch_long, is_fraud\n",
    "\n",
    "#find unique merchants\n",
    "unique_merchants = set(list(training_data_clean[\"merchant\"]))\n",
    "for merchant in list(test_data_clean[\"merchant\"]):\n",
    "    unique_merchants.add(merchant)\n",
    "unique_merchants = list(unique_merchants)\n",
    "print(len(unique_merchants))\n",
    "\n",
    "#create tokenization scheme for merchants\n",
    "merchant_tokenized_dict = {}\n",
    "for idx in range(len(unique_merchants)):\n",
    "    merchant_tokenized_dict[unique_merchants[idx]] = idx\n",
    "\n",
    "#find unique categories\n",
    "unique_categories = set(list(training_data_clean[\"category\"]))\n",
    "for category in list(test_data_clean[\"category\"]):\n",
    "    unique_categories.add(category)\n",
    "unique_categories = list(unique_categories)\n",
    "print(len(unique_categories))\n",
    "\n",
    "#create tokenization scheme for categories\n",
    "categories_tokenized_dict = {}\n",
    "for idx in range(len(unique_categories)):\n",
    "    categories_tokenized_dict[unique_categories[idx]] = idx\n",
    "\n",
    "#columns to keep: merchant, category, amt, lat, long, unix_time, merch_lat, merch_long, is_fraud\n",
    "#create non-categorical data list\n",
    "non_categorical = []\n",
    "for idx in range(len(training_data_clean[\"cc_num\"])):\n",
    "    element = []\n",
    "    #element.append(training_data_clean[\"cc_num\"][idx])\n",
    "    element.append(training_data_clean[\"amt\"][idx])\n",
    "    element.append(training_data_clean[\"lat\"][idx])\n",
    "    element.append(training_data_clean[\"long\"][idx])\n",
    "    element.append(training_data_clean[\"unix_time\"][idx])\n",
    "    element.append(training_data_clean[\"merch_lat\"][idx])\n",
    "    element.append(training_data_clean[\"merch_long\"][idx])\n",
    "    non_categorical.append(element)\n",
    "print(\"Non-categorical data length:\", len(non_categorical))\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "#format merchant column as one-hot list\n",
    "num_unique_merchants = len(unique_merchants)\n",
    "merchant_categorical = []\n",
    "for idx in range(len(training_data_clean[\"merchant\"])):\n",
    "    element_raw = training_data_clean[\"merchant\"][idx]\n",
    "    element_int = merchant_tokenized_dict[element_raw]\n",
    "    merchant_categorical.append(to_categorical(element_int, num_classes=num_unique_merchants))\n",
    "print(\"Merchant data length:\", len(merchant_categorical))\n",
    "\n",
    "#format category data as one-hot list\n",
    "num_unique_categories = len(unique_categories)\n",
    "category_one_hot = []\n",
    "for idx in range(len(training_data_clean[\"category\"])):\n",
    "    element_raw = training_data_clean[\"category\"][idx]\n",
    "    element_int = categories_tokenized_dict[element_raw]\n",
    "    category_one_hot.append(to_categorical(element_int, num_classes=num_unique_categories))\n",
    "print(\"Category length:\", len(category_one_hot))\n",
    "\n",
    "#create is_fraud list as y column\n",
    "fraud_column = []\n",
    "num_fraud = 0\n",
    "for idx in range(len(training_data_clean[\"is_fraud\"])):\n",
    "    fraud_val = training_data_clean[\"is_fraud\"][idx]\n",
    "    num_fraud += fraud_val\n",
    "    fraud_column.append([fraud_val])\n",
    "print(\"Fraud length:\", len(fraud_column))\n",
    "print(\"Number of fraudulent transactions:\", num_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-categorical shape: (14970, 6)\n",
      "Merchant shape: (14970, 693)\n",
      "Category shape: (14970, 14)\n",
      "Label shape: (14970, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "##find all fraud in training data\n",
    "fraud_data_merchant_sample = []\n",
    "fraud_data_category_sample = []\n",
    "fraud_data_non_categorical_sample = []\n",
    "fraud_data_label_sample = []\n",
    "for idx in range(len(training_data_clean[\"is_fraud\"])):\n",
    "    #if the row at idx represents fraud\n",
    "    if(training_data_clean[\"is_fraud\"][idx] == 1):\n",
    "        #add data to fraud data samples according to data type\n",
    "        fraud_data_non_categorical_sample.append(non_categorical[idx])\n",
    "        fraud_data_category_sample.append(category_one_hot[idx])\n",
    "        fraud_data_merchant_sample.append(merchant_categorical[idx])\n",
    "        fraud_data_label_sample.append([1])\n",
    "##add subsample of non-fraud data with equal number of rows to fraud data in training data\n",
    "max_index = len(training_data_clean[\"is_fraud\"]) - 1\n",
    "num_safe_transactions_included = 0\n",
    "while num_safe_transactions_included < num_fraud:\n",
    "    #select random row index in the formatted fraud data above\n",
    "    random_index = random.randint(0, max_index)\n",
    "    #if this data does not represent a fraudulent transaction\n",
    "    if(training_data_clean[\"is_fraud\"][random_index] == 0):\n",
    "        #add the data to fraud_data samples accordingly\n",
    "        fraud_data_non_categorical_sample.append(non_categorical[random_index])\n",
    "        fraud_data_category_sample.append(category_one_hot[random_index])\n",
    "        fraud_data_merchant_sample.append(merchant_categorical[random_index])\n",
    "        fraud_data_label_sample.append([0])\n",
    "    num_safe_transactions_included += 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fraud_data_non_categorical_sample = np.array(fraud_data_non_categorical_sample)\n",
    "print(\"Non-categorical shape:\", fraud_data_non_categorical_sample.shape)\n",
    "fraud_data_merchant_sample = np.array(fraud_data_merchant_sample)\n",
    "print(\"Merchant shape:\", fraud_data_merchant_sample.shape)\n",
    "fraud_data_category_sample = np.array(fraud_data_category_sample)\n",
    "print(\"Category shape:\", fraud_data_category_sample.shape)\n",
    "fraud_data_label_sample = np.array(fraud_data_label_sample)\n",
    "print(\"Label shape:\", fraud_data_label_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ML libraries\n",
    "import hyperopt\n",
    "\n",
    "from hyperopt import fmin, tpe\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Dropout, Input, concatenate\n",
    "\n",
    "from keras import activations\n",
    "from keras import callbacks\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#define method for generating model\n",
    "def define_model(num_nodes, dropout, kernel_reg, bias_reg):\n",
    "    #define input nodes\n",
    "    input_non_categorical = Input(shape=(6,))\n",
    "    input_merchant_category = Input(shape=(693,))\n",
    "    input_purchase_category = Input(shape=(14,))\n",
    "    #define processing\n",
    "    concat_input = concatenate(inputs=[input_non_categorical, input_merchant_category, input_purchase_category])\n",
    "    dense_layer_1 = Dense(num_nodes, activation=activations.relu, kernel_regularizer=regularizers.l2(kernel_reg), bias_regularizer=regularizers.l2(bias_reg))(concat_input)\n",
    "    dropout_layer_1 = Dropout(dropout)(dense_layer_1)\n",
    "    #dense_layer_2 = Dense(num_nodes // 2, activation=activations.relu, kernel_regularizer=regularizers.l2(kernel_reg), bias_regularizer=regularizers.l2(bias_reg))(dropout_layer_1)\n",
    "    #concat_tail = concatenate(inputs=[dropout_layer_1, input_non_categorical, input_purchase_category])\n",
    "    #dropout_layer_2 = Dropout(dropout / 2)(concat_tail)\n",
    "    #dense_layer_3 = Dense(num_nodes // 2, activation=activations.relu, kernel_regularizer=regularizers.l2(kernel_reg), bias_regularizer=regularizers.l2(bias_reg))(dropout_layer_2)\n",
    "    #define output\n",
    "    #output = Dense(1, activation=activations.sigmoid)(concat_tail)\n",
    "    output = Dense(1, activation=activations.sigmoid)(dropout_layer_1)\n",
    "    #compile and return\n",
    "    network = Model(inputs=[input_non_categorical, input_merchant_category, input_purchase_category], outputs=output)\n",
    "    #network = Model(inputs=[input_non_categorical, input_purchase_category], outputs=output)\n",
    "    network.compile(loss=\"binary_crossentropy\")\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [23:11<00:00, 10.87s/trial, best loss: 0.685332453250885]\n",
      "Best parameters: (32, 0.2, 0.025, 0.025)\n",
      "Best loss: 0.685332453250885\n"
     ]
    }
   ],
   "source": [
    "#do hyperparameter tuning here\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "best_loss = math.inf\n",
    "best_params = ()\n",
    "\n",
    "def average(data_list):\n",
    "    avg = 0\n",
    "    for idx in range(len(data_list)):\n",
    "        if not np.isnan(data_list[idx]):\n",
    "            avg += data_list[idx]\n",
    "        else:\n",
    "            avg += 10\n",
    "    avg /= len(data_list)\n",
    "    return avg\n",
    "\n",
    "def get_average_cross_validation_accuracy(args):\n",
    "    global best_loss, best_params\n",
    "    max_node_count, dropout, kernel_regularization, bias_regularization = args\n",
    "    data_split = KFold(n_splits=5, shuffle=True)\n",
    "    loss_vals = []\n",
    "    callbacks_list = [callbacks.EarlyStopping(monitor='val_loss', patience=5,), callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=1,)]\n",
    "    for train_index, test_index in data_split.split(fraud_data_non_categorical_sample, fraud_data_label_sample):\n",
    "        #convert data into train and test data\n",
    "        non_categorical_train, non_categorical_test = fraud_data_non_categorical_sample[train_index], fraud_data_non_categorical_sample[test_index]\n",
    "        merchant_cat_train, merchant_cat_test = fraud_data_merchant_sample[train_index], fraud_data_merchant_sample[test_index]\n",
    "        purchase_cat_train, purchase_cat_test = fraud_data_category_sample[train_index], fraud_data_category_sample[test_index]\n",
    "        label_train, label_test = fraud_data_label_sample[train_index], fraud_data_label_sample[test_index]\n",
    "        network = define_model(max_node_count, dropout, kernel_regularization, bias_regularization)\n",
    "        network.fit(x=[non_categorical_train, merchant_cat_train, purchase_cat_train], y=label_train, epochs=20, batch_size=128, validation_split=0.2, callbacks=callbacks_list, verbose=0)\n",
    "        evaluation = network.evaluate(x=[non_categorical_test, merchant_cat_test, purchase_cat_test], y=label_test, verbose=0)\n",
    "        #network.fit(x=[non_categorical_train, purchase_cat_train], y=label_train, epochs=20, batch_size=128, validation_split=0.2, callbacks=callbacks_list, verbose=0)\n",
    "        #evaluation = network.evaluate(x=[non_categorical_test, purchase_cat_test], y=label_test, verbose=0)\n",
    "        loss_vals.append(evaluation)\n",
    "        del network\n",
    "        del non_categorical_train, non_categorical_test\n",
    "        del merchant_cat_train, merchant_cat_test\n",
    "        del label_train, label_test\n",
    "    average_loss = average(loss_vals)\n",
    "    if average_loss < best_loss:\n",
    "        best_params = args\n",
    "        best_loss = average_loss\n",
    "    return average_loss\n",
    "\n",
    "def optimize_hyperparameters():\n",
    "    max_node_counts = [32, 64, 128, 256, 512]\n",
    "    dropout_values = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "    kernel_reg_list = [0.01, 0.025, 0.05]\n",
    "    bias_reg_list = [0.01, 0.025, 0.05]\n",
    "    case_list = []\n",
    "    for max_node_count in max_node_counts:\n",
    "        for dropout_value in dropout_values:\n",
    "            for kernel_reg in kernel_reg_list:\n",
    "                for bias_reg in bias_reg_list:\n",
    "                    case_list.append((max_node_count, dropout_value, kernel_reg, bias_reg))\n",
    "    feature_space = hyperopt.hp.choice('a', case_list)\n",
    "    fmin(get_average_cross_validation_accuracy, feature_space, algo=tpe.suggest, max_evals=128)\n",
    "\n",
    "optimize_hyperparameters()\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11976 samples, validate on 2994 samples\n",
      "Epoch 1/20\n",
      "11976/11976 [==============================] - 1s 107us/step - loss: 6999204.8599 - val_loss: 2048748.4890\n",
      "Epoch 2/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 1249072.1383 - val_loss: 0.0266\n",
      "Epoch 3/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 29302.3100 - val_loss: 0.0229\n",
      "Epoch 4/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 2000.6421 - val_loss: 1876.2998\n",
      "Epoch 5/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 1398.7321 - val_loss: 3353.5233\n",
      "Epoch 6/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 820.8241 - val_loss: 0.0207\n",
      "Epoch 7/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 786.7415 - val_loss: 0.0204\n",
      "Epoch 8/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 954.4176 - val_loss: 4143.0151\n",
      "Epoch 9/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 673.5271 - val_loss: 664.8245\n",
      "Epoch 10/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 361.9006 - val_loss: 0.0194\n",
      "Epoch 11/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 199.5560 - val_loss: 258.3486\n",
      "Epoch 12/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 143.8127 - val_loss: 0.0188\n",
      "Epoch 13/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 91.5522 - val_loss: 0.0184\n",
      "Epoch 14/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 71.6738 - val_loss: 0.0181\n",
      "Epoch 15/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 39.3162 - val_loss: 31.4842\n",
      "Epoch 16/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 10.4157 - val_loss: 12.2996\n",
      "Epoch 17/20\n",
      "11976/11976 [==============================] - 0s 35us/step - loss: 2.3373 - val_loss: 9.8840\n",
      "Epoch 18/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 1.2920 - val_loss: 0.4314\n",
      "Epoch 19/20\n",
      "11976/11976 [==============================] - 0s 36us/step - loss: 0.9896 - val_loss: 1.0387\n"
     ]
    }
   ],
   "source": [
    "#Train neural network here\n",
    "import os\n",
    "#os.mkdir(\"cc_fraud_detector\")\n",
    "\n",
    "callbacks_list = [callbacks.EarlyStopping(monitor='val_loss', patience=5,)]\n",
    "try:\n",
    "    max_node_count, dropout_value, kernel_reg, bias_reg = best_params\n",
    "except:\n",
    "    max_node_count, dropout_value, kernel_reg, bias_reg = 32, 0.2, 0.025, 0.025\n",
    "\n",
    "network = define_model(max_node_count, dropout_value, kernel_reg, bias_reg)\n",
    "history = network.fit(x=[fraud_data_non_categorical_sample, fraud_data_merchant_sample, fraud_data_category_sample], y=fraud_data_label_sample, epochs=20, batch_size=128, validation_split=0.2, callbacks=callbacks_list)\n",
    "#history = network.fit(x=[fraud_data_non_categorical_sample, fraud_data_category_sample], y=fraud_data_label_sample, epochs=20, batch_size=128, validation_split=0.2, callbacks=callbacks_list)\n",
    "plot_model(network, to_file='cc_fraud_detector.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False positive and negative rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1296675/1296675 [==============================] - 8s 6us/step\n",
      "True positives: 7506\n",
      "True negatives: 0\n",
      "False positives: 1289169\n",
      "False negatives: 0\n"
     ]
    }
   ],
   "source": [
    "#make predictions based on ALL data rows (1296675 rows)\n",
    "prediction = network.predict([non_categorical, merchant_categorical, category_one_hot], batch_size=128, verbose=1, use_multiprocessing=True)\n",
    "prediction = prediction.reshape((1296675))\n",
    "prediction = prediction.tolist()\n",
    "truth_values = np.array(training_data_clean[\"is_fraud\"]).reshape((1296675))\n",
    "truth_values = truth_values.tolist()\n",
    "#create counts for false pos, false negative, true pos, and true negative\n",
    "false_pos_count = 0\n",
    "false_negative_count = 0\n",
    "true_pos_count = 0\n",
    "true_negative_count = 0\n",
    "#for each prediction\n",
    "for idx in range(1296675):\n",
    "    #compare each prediction to the true value\n",
    "    #increment appropriate count\n",
    "    if(abs(prediction[idx] - truth_values[idx]) < 0.5):\n",
    "        if(int(truth_values[idx]) == 0):\n",
    "            true_negative_count += 1\n",
    "        else:\n",
    "            true_pos_count += 1\n",
    "    else:\n",
    "        if(int(truth_values[idx]) == 0):\n",
    "            false_pos_count += 1\n",
    "        else:\n",
    "            false_negative_count += 1\n",
    "#output\n",
    "print(\"True positives:\", true_pos_count)\n",
    "print(\"True negatives:\", true_negative_count)\n",
    "print(\"False positives:\", false_pos_count)\n",
    "print(\"False negatives:\", false_negative_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0415509112926966\n",
      "{'val_loss': [986944.9451402805, 20466.413291426605, 0.017238952219486237, 15944.724000735847, 0.01613517850637436, 13362.053722027387, 294.20599477356404, 0.015318620949983597, 2466.092094997286, 3440.411817123831, 0.014605141244828701, 0.014344729483127594, 185.6782540096469, 27.894352156716185, 1.435651861673685, 1.3124525739737327, 1.0449346056283915], 'loss': [7020387.8559619235, 304408.5956783358, 11525.634831348896, 5185.692391129916, 2840.280045523991, 1876.2393422913535, 1310.6587496281625, 1702.8371943268046, 1101.9295978839189, 932.8935347093927, 410.93460947366736, 189.7249097906914, 71.31185670494635, 42.09237258818123, 1.5221927117807672, 0.7182963884665159, 0.746050506571411]}\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "print(network.evaluate([non_categorical, merchant_categorical, category_one_hot], np.array(training_data_clean[\"is_fraud\"]), verbose=0))\n",
    "print(history.history)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4a56ac7d8a0ccd50127e345ccc30f7d61dbad599a11a46d4752f0ccdb34e3a5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
